{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch as pt\n",
    "from torch import nn\n",
    "\n",
    "print(f\"Torch version: {pt.__version__}\")\n",
    "\n",
    "# if pt.cuda.is_available():\n",
    "#     device = 'cuda'\n",
    "# if pt.backends.mps.is_available():\n",
    "#     device = 'mps'\n",
    "# else:\n",
    "#     device= 'cpu'\n",
    "device = 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists\n",
      "/Users/gustavgamstedt/Desktop/github to hemma/PyTorch/04/data/pizza_steak_sushi.zip doesn't exist, download\n",
      "Unzipping pizza, steak and sushi data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import importLib\n",
    "from sys import path\n",
    "import zipfile\n",
    "\n",
    "\n",
    "# Create directory\n",
    "data_path = Path(f\"{path[0]}/data\")\n",
    "image_path = data_path / 'pizza_steak_sushi'\n",
    "if image_path.exists():\n",
    "    print('Already exists')\n",
    "else:\n",
    "    image_path.mkdir(parents=True)\n",
    "\n",
    "\n",
    "# Download pizza, steak and sushi data\n",
    "# open skapar en zip fil som sedan fylls genom request\n",
    "importLib.import_from_github('https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip',directory=data_path)\n",
    "with zipfile.ZipFile(data_path/'pizza_steak_sushi.zip', 'r') as zip_ref:\n",
    "    print('Unzipping pizza, steak and sushi data')\n",
    "    zip_ref.extractall(image_path)\n",
    "Path.unlink(data_path/'pizza_steak_sushi.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/gustavgamstedt/Desktop/github to hemma/PyTorch/04/data/pizza_steak_sushi/train'),\n",
       " PosixPath('/Users/gustavgamstedt/Desktop/github to hemma/PyTorch/04/data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup training and testing paths\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root = train_dir, transform=simple_transform)\n",
    "test_dataset = datasets.ImageFolder(root = test_dir, transform=simple_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = round(os.cpu_count()*(3/4))\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ModelWithoutAugmentation(nn.Module):\n",
    "    def __init__(self, input_features:int,output_features:int, hidden_units:int=10):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_features, hidden_units,\n",
    "                      kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.Conv2d(hidden_units, hidden_units,\n",
    "                      kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units,\n",
    "                      kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.Conv2d(hidden_units, hidden_units,\n",
    "                      kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_units*13*13, output_features)\n",
    "        )\n",
    "    def forward(self, X:pt.Tensor) -> pt.Tensor:\n",
    "        X_change = self.conv_block_1(X)\n",
    "        X_change = self.conv_block_2(X_change)\n",
    "        # print(X_change.shape)\n",
    "        X_change = self.classifier(X_change)\n",
    "        return X_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelWithoutAugmentation(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.manual_seed(42)\n",
    "model0 = ModelWithoutAugmentation(input_features=3, output_features=len(train_dataset.classes), hidden_units=10).to(device)\n",
    "model0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64]) 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7012e-02, -5.3149e-03,  1.2772e-02],\n",
       "        [ 1.8558e-02, -1.8290e-03,  9.5356e-03],\n",
       "        [ 2.1909e-02, -4.9545e-03,  9.4219e-03],\n",
       "        [ 2.3536e-02, -5.1556e-03,  1.0510e-02],\n",
       "        [ 2.1787e-02,  6.3863e-05,  7.6012e-03],\n",
       "        [ 2.0530e-02, -1.1189e-03,  1.1084e-02],\n",
       "        [ 2.1927e-02, -1.7779e-03,  8.1963e-03],\n",
       "        [ 2.0188e-02, -2.4430e-03,  1.0628e-02],\n",
       "        [ 2.2015e-02, -1.4427e-03,  7.8839e-03],\n",
       "        [ 2.1256e-02, -1.6102e-03,  9.5531e-03],\n",
       "        [ 1.9500e-02, -2.4019e-03,  8.3024e-03],\n",
       "        [ 2.1457e-02, -1.8471e-03,  7.7899e-03],\n",
       "        [ 2.0759e-02,  1.9331e-04,  7.7705e-03],\n",
       "        [ 2.1412e-02, -1.9498e-03,  1.2461e-02],\n",
       "        [ 1.8300e-02,  1.5043e-03,  1.0491e-02],\n",
       "        [ 2.2685e-02, -2.1673e-03,  9.2847e-03],\n",
       "        [ 2.3170e-02, -1.5725e-03,  8.5609e-03],\n",
       "        [ 2.0840e-02, -1.9240e-03,  9.4237e-03],\n",
       "        [ 2.2364e-02, -1.1584e-03,  7.3213e-03],\n",
       "        [ 2.3398e-02, -3.6900e-03,  1.0476e-02],\n",
       "        [ 2.4240e-02, -2.8747e-04,  1.0837e-02],\n",
       "        [ 2.1274e-02, -3.5763e-03,  8.7465e-03],\n",
       "        [ 2.0510e-02, -2.5316e-03,  8.4688e-03],\n",
       "        [ 2.0908e-02, -8.5384e-04,  9.1636e-03],\n",
       "        [ 2.2970e-02, -3.8665e-03,  9.7433e-03],\n",
       "        [ 1.9975e-02, -1.6059e-03,  7.8182e-03],\n",
       "        [ 2.2101e-02, -4.1470e-03,  9.3903e-03],\n",
       "        [ 1.9523e-02, -4.7900e-04,  7.5137e-03],\n",
       "        [ 2.0531e-02, -3.9331e-03,  1.0121e-02],\n",
       "        [ 1.9832e-02, -1.3674e-03,  1.1364e-02],\n",
       "        [ 1.8615e-02, -4.1769e-03,  9.5063e-03],\n",
       "        [ 2.1606e-02, -4.0847e-03,  1.0492e-02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(iter(train_dataloader))\n",
    "print(imgs.shape, len(labels))\n",
    "# model0(imgs[0].unsqueeze(0))\n",
    "model0(imgs.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchinfo\n",
    "except ModuleNotFoundError:\n",
    "    print('Module not found, installing module')\n",
    "    !pip3 install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ModelWithoutAugmentation                 [32, 3]                   --\n",
       "├─Sequential: 1-1                        [32, 10, 30, 30]          --\n",
       "│    └─Conv2d: 2-1                       [32, 10, 62, 62]          280\n",
       "│    └─ReLU: 2-2                         [32, 10, 62, 62]          --\n",
       "│    └─Conv2d: 2-3                       [32, 10, 60, 60]          910\n",
       "│    └─ReLU: 2-4                         [32, 10, 60, 60]          --\n",
       "│    └─MaxPool2d: 2-5                    [32, 10, 30, 30]          --\n",
       "├─Sequential: 1-2                        [32, 10, 13, 13]          --\n",
       "│    └─Conv2d: 2-6                       [32, 10, 28, 28]          910\n",
       "│    └─ReLU: 2-7                         [32, 10, 28, 28]          --\n",
       "│    └─Conv2d: 2-8                       [32, 10, 26, 26]          910\n",
       "│    └─ReLU: 2-9                         [32, 10, 26, 26]          --\n",
       "│    └─MaxPool2d: 2-10                   [32, 10, 13, 13]          --\n",
       "├─Sequential: 1-3                        [32, 3]                   --\n",
       "│    └─Flatten: 2-11                     [32, 1690]                --\n",
       "│    └─Linear: 2-12                      [32, 3]                   5,073\n",
       "==========================================================================================\n",
       "Total params: 8,083\n",
       "Trainable params: 8,083\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 181.95\n",
       "==========================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 22.80\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 24.40\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model0, input_size=[32,3,64,64],device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def train_step(model: pt.nn.Module, \n",
    "               dataloader:DataLoader, \n",
    "               loss_fn: pt.nn.Module, \n",
    "               optimizer:pt.optim.Optimizer, \n",
    "               device:pt.device, \n",
    "               show:bool=False):\n",
    "    \"\"\"Performs a training step with model trying to learn on data_loader\n",
    "\n",
    "    args:\n",
    "        model: the model which will be trained on\n",
    "        dataloader: A generator like loader for the data\n",
    "        optimizer: Optimizer which optimizes the code through gradient descend\n",
    "        loss_fn: function which calculates how far from the right answer each of the predictions were\n",
    "        accuracy_fn: function which calculates how meny predictions were right\n",
    "        device: chosen device for the neural network to run on (cpu/gpu/tpu)\n",
    "        show: if true display the loss and acc in console \n",
    "        \n",
    "    returns:\n",
    "        (loss, accuracy)\"\"\"\n",
    "    # Put model in training mode\n",
    "    model.train()\n",
    "     \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0,0\n",
    "\n",
    "    # Loop through data loader batches\n",
    "    for X,y in dataloader:\n",
    "        # Send data to target device\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        y_logits = model(X)\n",
    "        \n",
    "        loss = loss_fn(y_logits, y)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_preds = pt.argmax(pt.softmax(y_logits, dim=1), dim=1) # Softmax is actually unnecessary, but can be useful for visualization and also to give completeness\n",
    "        train_acc += pt.eq(y_preds, y).sum().item()/len(y_preds)\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc  /= len(dataloader)\n",
    "\n",
    "    if show:\n",
    "        print(f'Train loss: {train_loss} | Train acc: {train_acc}')\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: pt.nn.Module, \n",
    "              dataloader:DataLoader, \n",
    "              loss_fn: pt.nn.Module, \n",
    "              device:pt.device, \n",
    "              show:bool=False):\n",
    "    \"\"\"Performs a testing loop step on model going over data_loader.\n",
    "\n",
    "    args:\n",
    "        model: the model which will be trained on\n",
    "        dataloader: A generator like loader for the data\n",
    "        loss_fn: function which calculates how far from the right answer each of the predictions were\n",
    "        accuracy_fn: function which calculates how meny predictions were right\n",
    "        device: chosen device for the neural network to run on (cpu/gpu/tpu)\n",
    "        show: if true display the loss and acc in console \n",
    "\n",
    "    returns:\n",
    "        (loss, accuracy)\"\"\"\n",
    "    test_acc, test_loss = 0,0\n",
    "    \n",
    "    model.eval()\n",
    "    with pt.inference_mode():\n",
    "        for X,y in dataloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_logits = model(X)\n",
    "            loss = loss_fn(y_logits, y)\n",
    "            test_loss+=loss.item()\n",
    "\n",
    "            y_preds = pt.argmax(pt.softmax(y_logits, dim=1), dim=1)\n",
    "            test_acc += pt.eq(y_preds, y).sum().item()/len(y_preds)\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc  /= len(dataloader)\n",
    "    if show:\n",
    "        print(f'Test loss: {test_loss} | Test acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = pt.optim.Adam(model0.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 1.214791752398014 | Train acc: 0.3046875\n",
      "Test loss: 1.1588222185770671 | Test acc: 0.2604166666666667\n",
      "Epoch: 1\n",
      "Train loss: 1.105965219438076 | Train acc: 0.3046875\n",
      "Test loss: 1.1224364042282104 | Test acc: 0.2604166666666667\n",
      "Epoch: 2\n",
      "Train loss: 1.0975951552391052 | Train acc: 0.3046875\n",
      "Test loss: 1.1026708682378132 | Test acc: 0.2604166666666667\n",
      "Epoch: 3\n",
      "Train loss: 1.0952743589878082 | Train acc: 0.3046875\n",
      "Test loss: 1.0911263624827068 | Test acc: 0.2604166666666667\n",
      "Epoch: 4\n",
      "Train loss: 1.0944082736968994 | Train acc: 0.53125\n",
      "Test loss: 1.0834309260050456 | Test acc: 0.5416666666666666\n",
      "Timer: by GGisMee\n",
      "=================\n",
      "Interval time: \n",
      "Interval 1: 36.29\n",
      "Interval 2: 36.27\n",
      "Interval 3: 35.83\n",
      "Interval 4: 35.89\n",
      "Interval 5: 37.47\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ml_funcs import Model_operations, Timer\n",
    "from tqdm.notebook import tqdm\n",
    "epochs = 5\n",
    "timer = Timer()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    Model_operations.train_step(\n",
    "        model=model0,\n",
    "        dataloader=train_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        show=True)\n",
    "    Model_operations.test_step(model0, test_dataloader, loss_fn, device, True)\n",
    "    timer.interval()\n",
    "\n",
    "timer.show_as_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'ModelWithoutAugmentation', 'model_loss': 1.0938726663589478, 'model_acc': 0.40234375}\n"
     ]
    }
   ],
   "source": [
    "print(Model_operations.eval_model(model0, train_dataloader, loss_fn, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
