{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Neural Network classification with PyTorch\n",
    "\n",
    "Classification is a problem of predicting whether something is one thing or another (there can be multiple things as options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "from sys import path\n",
    "import pandas as pd\n",
    "\n",
    "def df_to_t(df):\n",
    "    return pt.from_numpy(df.to_numpy()).type(pt.float32)\n",
    "\n",
    "#print(f\"{path[0]}\\ds.csv\")\n",
    "DataFrame = pd.read_csv(f\"{path[0]}\\ds.csv\")\n",
    "X = df_to_t(DataFrame.iloc[:,1:3])\n",
    "y = df_to_t(DataFrame.iloc[:,3:])\n",
    "y = (y.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "plt.scatter(X[:,0], \n",
    "            X[:,1],\n",
    "            c=y,\n",
    "            cmap=plt.cm.RdYlBu)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape) # betyder att 2 X värdet (input) blir 1 y värde (output)\n",
    "\n",
    "# i en grafisk mening är inputen X = (x,y)\n",
    "# och y blir en ny variabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first example of features and labels\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "print(f\"One sample of X: {X_sample}, one sample of y: {y_sample}\")\n",
    "print(f\"Shape of one sample of X: {X_sample.shape}, one sample of y: {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mlen\u001b[39m(X_train), \u001b[39mlen\u001b[39m(X_test), \u001b[39mlen\u001b[39m(y_train), \u001b[39mlen\u001b[39m(y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Goal: Build a model to clarify which is blue and which is red / 0 or 1\n",
    "\n",
    "To do so, we want to:\n",
    "1. Setup device agonistic code so our code will run on an accelerator (GPU) if there is one\n",
    "2. Construct a model (by subclassing 'nn.Module')\n",
    "3. Define loss and optimizer functions\n",
    "4. Create training and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device agnostic code\n",
    "device = \"cuda\" if pt.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct model:\n",
    "1. Subclass nn.module\n",
    "2. Create 2 nn.Linear() layers that are capable of handling the shapes of our data\n",
    "3. Define a forward method\n",
    "4. Instatiate an instance of our model class and send it to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "pt.manual_seed(42)\n",
    "# Consturct the model\n",
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        #2. Create 2 nn.Linear layers\n",
    "        self.Layer_1 = nn.Linear(in_features=2, out_features=5) # nu ger out_features ut 5 noder vilka sedan går in i layer_2:s in_features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # same shape as y\n",
    "    \n",
    "    # 3. Define a forward() method that outlines the forward pass\n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.Layer_1(x)) # x -> layer 1 -> layer 2 -> returned as output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an instance of our modle class\n",
    "model_0 = CircleModelV1().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replicate the model above using nn.Sequential(), basically en förkortning (sämre för mer komplicerade)\n",
    "# pt.manual_seed(42)\n",
    "# model_0 = nn.Sequential(\n",
    "#     nn.Linear(in_features=2, out_features=5),\n",
    "#     nn.Linear(in_features=5, out_features=1)\n",
    "# ).to(device)\n",
    "# print(model_0, model_0.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pt.inference_mode():\n",
    "    untrained_preds = model_0(X_test.to(device))\n",
    "    print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "    print(f\"Length of Test samples: {len(X_test)}, Shape: {X_test.shape}\")\n",
    "    print()\n",
    "    print(f\"First 10 predictions: {pt.round(untrained_preds[:10])}\")\n",
    "    print(f\"First 10 Test Lables: {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and optimizer function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup loss function and optimizer\n",
    "\n",
    "Which loss function or optimizer should you use now?\n",
    "\n",
    "For regression problems you might want to use MAE or MSE (mean absolute error or mean squared error)\n",
    "\n",
    "For classification you might want binary cross entropy or categorical cross entropy \n",
    "\n",
    "For this loss function we are going to use 'torch.nn.BECWithLogitsLoss()': https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "Definition of logit in deep learning: https://saturncloud.io/blog/what-is-the-meaning-of-the-word-logits-in-tensorflow/\n",
    "\n",
    "Sigmoid activation function: https://www.learndatasci.com/glossary/sigmoid-function/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss function\n",
    "#~ loss_fn = nn.BCEWithLogitsLoss() # requires inputs to have gone through the sigmoid activation function prior to input to BCELoss\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss has tha sigmoid activation function built in\n",
    " \n",
    "optimizer = pt.optim.SGD(params=model_0.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, how meny of the the examples the model gets right\n",
    "def accuracy_fn(y_true, y_preds):\n",
    "    correct = pt.eq(y_true, y_preds).sum().item()\n",
    "    acc = (correct/len(y_preds)) * 100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n",
    "\n",
    "To train our model, we are going to need to build a training loop:\n",
    "\n",
    "1. Forward pass\n",
    "2. Calculate the loss \n",
    "3. Optimizer zero grad\n",
    "4. Loss backwards (backpropagation)\n",
    "5. Optimizer (gradient descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from raw logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "Our model outputs are going to be raw **logits**.\n",
    "\n",
    "We can convert these logits into prediction probabilities by passing them through some kind to activation function (e.g sigmoid for binary cross entropy and softmax for  multpclass classification)\n",
    "\n",
    "Then we can convert our model's prediction probabilities to **prediction labels** by either rounding them or taking the argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 ouputs of the forward pass on the test data\n",
    "model_0.eval() # när man gör förutsägelser behöver man använda eval mode\n",
    "with pt.inference_mode():\n",
    "    y_logits = model_0(X_test.to(device)[:5])\n",
    "    print(\"These are not equal and cannot be paired as one is binary and the other are floats\")\n",
    "    print(f\"y test logits: {y_logits}\")\n",
    "    print(f\"y test optimal: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sigmoid activation function on our model logits to turn them into prediction probabilities\n",
    "y_pred_probs = pt.sigmoid(y_logits)\n",
    "print(f\"Through the sigmoid function gives: {y_pred_probs}\")\n",
    "print()\n",
    "print(f\"When rounded they render in the same binary format as with the y test\")\n",
    "y_preds = pt.round(y_pred_probs).squeeze()\n",
    "print(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our prediction probability values, we need to perform a range-style rounding on them\n",
    "* y_pred_probs >= 0.5 then y=1\n",
    "* y_pred_probs < 0.5 then y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in full, (logits -> pred probs -> pred labels)\n",
    "model_0.eval()\n",
    "with pt.inference_mode():\n",
    "    y_preds_labels = pt.round(pt.sigmoid(model_0(X_test.to(device))))\n",
    "    \n",
    "    y_preds_labels = y_preds_labels.squeeze()\n",
    "    print(f\"{y_preds_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.manual_seed(42)\n",
    "pt.cuda.manual_seed(42) # om man använder en gpu\n",
    "\n",
    "# target the data to selected device\n",
    "X_train,y_train = X_train.to(device), y_train.to(device)\n",
    "X_test,y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training:\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_preds = pt.round(pt.sigmoid(y_logits)) # logits -> pred probs -> pred labels\n",
    "    \n",
    "    # 2. Calculate the loss/accuracy \n",
    "    loss = loss_fn(y_logits, y_train) # eftersom den är BCEWithlogitLoss()\n",
    "\n",
    "    acc = accuracy_fn(y_train, y_preds)\n",
    "\n",
    "    # om den var BCELoss\n",
    "    # loss_fn(pt.sigmoid(y_logits), y_train)\n",
    "\n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step (gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with pt.inference_mode():\n",
    "        # 1. forward pass\n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_pred = pt.round(pt.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate test loss/acc\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_preds=test_pred)\n",
    "    # Print out whats happening\n",
    "    if epoch%10==0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}, Acc: {acc}, Test loss: {test_loss}, Test accuracy: {test_acc:}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make predictions and evaluate the model in a vicualization\n",
    "\n",
    "From our metrics it looks like our model isn't learning anything\n",
    "\n",
    "So to inspect it let's make some predictions and make them visual!\n",
    "\n",
    "In other words, \"Visualize, Visualize, Visualize!\"\n",
    "\n",
    "To do so, we're going to import a function called 'lot_decision_boundary()'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import helper functions\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper function already exists\")\n",
    "else:\n",
    "    print(\"helper functions doesn't exist, download\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        f.write(request.content)\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary of the model\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving a model (from a model perspective)\n",
    "\n",
    "#### **These values which change the functionality are called hyperparameters:**\n",
    "* Add more layers - give the model more chances to learn about patterns\n",
    "* Add more neurons/nodes\n",
    "* Train for longer\n",
    "* Change the activation function, using sigmoid rn\n",
    "* Change the learning rate\n",
    "* Change the loss function\n",
    "\n",
    "#### There are also way to improve the model from a data perspective\n",
    "* e.g increase the size of dataset   #e.g = example wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve our model by:\n",
    "* adding more hidden units/nodes: 5 -> 10\n",
    "* Increase the number of layers: 2 -> 3\n",
    "* Increase the number of epochs: 100 -> 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you want to find a way to improve your model, you often change one hyperparamater at a time, so you know which one may increase effectivness and which one decreases instead\n",
    "pt.manual_seed(42)\n",
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "    def forward(self, x):\n",
    "        # option 1, går lite fortare\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "\n",
    "        # option 2:\n",
    "        # layer = self.layer_1(x)\n",
    "        # layer = self.layer_2(layer) # layer updateras\n",
    "        # layer = self.layer_3(layer)\n",
    "        # return layer\n",
    "model_1 = CircleModelV1().to(device)\n",
    "model_1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = pt.optim.SGD(params=model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a training and evaluation loop for model_1\n",
    "pt.manual_seed(42)\n",
    "pt.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "    \n",
    "    y_logits = model_1(X_train).squeeze()\n",
    "    y_preds = pt.round(pt.sigmoid(y_logits))\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_train, y_preds)\n",
    "\n",
    "    # 3 Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards (back propogation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step (gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_1.eval()\n",
    "    with pt.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_1(X_test).squeeze()\n",
    "        test_pred = pt.round(pt.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_test, test_pred)\n",
    "\n",
    "        # Print out what is happening\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Acc: {acc}, Test loss: {test_loss}, Test acc: {test_acc}%\")\n",
    "            \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_1, X_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_1, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - skipped, bc not necessary\n",
    "# 6 The missing piece: non-linearity\n",
    "What patterns could you draw if you had a high amount of staight and non-straight lines? or linear and non linear functions\n",
    "in doc 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
