{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Reinforcement Learning (RL) is an area of machine larning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward\n",
    "\n",
    "Simplified:\n",
    "RL is teaching a software agent (computer player) how to behave in an environment (the game) by telling it how good it's doing (reward). It then optimizes itself to reach as much reward as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "This approach extends reinforcement learning by using a deep neural network to predict the actions.\n",
    "\n",
    "Q Value = Quality of action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Agent (puts everything together)\n",
    "- game\n",
    "- model\n",
    "#### Training:\n",
    "- state = get_state(game)\n",
    "- action = get_move(state)\n",
    "    - model.predict()\n",
    "- reward, game_over, score = game.play_step(action)\n",
    "- new_state = get_state(game)\n",
    "- remember\n",
    "- model.train()\n",
    "\n",
    "### Game (Pygame)\n",
    "- play_step(action)\n",
    "    - reward, game_over, score\n",
    "\n",
    "### Model (Pytorch)\n",
    "Linear_QNet (DQN)\n",
    "- model.predict(state)\n",
    "    - action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward system\n",
    "- eat food +10 points\n",
    "- game over -10 points\n",
    "- else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    " \n",
    "[1,0,0] -> straight\n",
    "\n",
    "[0,1,0] -> right turn\n",
    "\n",
    "[0,0,1] -> left turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State (11 values)\n",
    "[danger straight, danger right, danger left\n",
    "\n",
    "direction left, direction right, direction up, direction down,\n",
    "\n",
    "\n",
    "food left, food right, food up, food down]\n",
    "\n",
    "This is displayed in an array ex. \n",
    "\n",
    "[0,0,0\n",
    "\n",
    "0,1,0,0,\n",
    "\n",
    "0,1,0,1]\n",
    "\n",
    "Which means that there exists there is no danger nearby, its moving right, there is food right down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "\n",
    "state -> models NN -> action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q learning\n",
    "Q Value = Quality of action\n",
    "\n",
    "0. Init Q Value (init model)\n",
    "1. Choose action (model.predict(state)), in the beginning random move is recommanded\n",
    "2. Perform action\n",
    "3. Measure reward\n",
    "4. Update Q value (train model) -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The switch between is called the trade off between exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Bellman Equation:\n",
    "NewQ(s,a) = Q(s,a) + lr[R(s,a)+ dr*maxQ'(s',a')-Q(s,a)]\n",
    "\n",
    "s = state, a = action\n",
    "lr = learning rate, dr = discount rate\n",
    "R = reward\n",
    "Q'(s',a') = Maximum expected future reward given the new s' and all possible actions at that new state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss = (Q_new-Q)^2 -> Mean Squared Error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
