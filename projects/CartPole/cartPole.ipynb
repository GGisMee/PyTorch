{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "### Reinforcement Learning (DQN) Tutorial text\n",
    "Link: https://h-huang.github.io/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# Set up matplotlib\n",
    "if is_ipython := 'inline' in matplotlib.get_backend():\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# If GPU is to be used\n",
    "device = pt.device('cuda' if pt.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experience replay memory for training our DQN. It stores the transitions that the agent observes, which can be reused later. By sampeling from it randomly the transitions that buld up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the training procedure.\n",
    "\n",
    "Two classes in use:\n",
    "* Transition - a named tuple represented by a single transition in our environment. It maps (state, action) to this paired (next_state, reward) result\n",
    "* ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method which can be used to select a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    '''ReplayMemory stores the transitions that the agent observes, allowing us to reuse this data later. \n",
    "    \n",
    "    By sampling from it randomly, the transitions that build up a batch are decorrelated.'''\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        '''Saves a transition'''\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. In the reinforcement learning literature, they would also contain expectations over stochastic transitions in the environment (randomness with a pattern).\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted, cumulative reward \n",
    "R_t0 = sum(discountRate * r_t)\n",
    "\n",
    "The main idea behind Q-learning is that if we have a function Q*: State*Action -> R, That couldtell us what our return would be if we were to take an action in a given state, then we could easily construct a policy that maximizes or rewards:\n",
    "\n",
    "pi*(s) = argmax Q*(s,a)\n",
    "\n",
    "However, we don't know everything in the world, so we dont have access to Q*, but we can approximate this. Therefore our updated training rule we can use this Q function\n",
    "\n",
    "Q^pi(s,a) = r+discount*Q^pi(s', pi(s'))\n",
    "\n",
    "To minimize the error we can use Huber Loss: https://en.wikipedia.org/wiki/Huber_loss\n",
    "Which acts like the mean sqeared error, when the error is small, but like the mean absolute error when its large making this method robust:\n",
    "\n",
    "Q-net\n",
    "Our model will be feed forward neural network that takes in the difference between the current and previous screen patches. It has two outputs, representing Q(s, left) or Q(s, right), where right or left is the action and s is the input to the network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_units:int, output_units:int, hidden_units:int=128) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_units, hidden_units)\n",
    "        self.layer2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.layer3 = nn.Linear(hidden_units, output_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams and utilities\n",
    "This cell instantiates our model and its optimizer and defines some utilities:\n",
    "\n",
    "* select_action - selects action according to epsilon greedy policy. The probability of choosing a random action will start at EPS_Start and move towards EPS_END where EPS_DECAY is the rate of decay. This creates a gradual transformation from exploration to exploitation\n",
    "* plot_durations - a helper for plotting the duration of episodes, along with the averate for the last 100 episodes (The measure used in the official evaluations). The plot underneath the cell containing the main training loop and will update every episode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "GAMMA = 0.99\n",
    "\n",
    "# EPS_START is the starting value of epsilon\n",
    "EPS_START = 0.9\n",
    "\n",
    "# EPS_END is the final value of epsilon\n",
    "EPS_END = 0.05\n",
    "\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# TAU is the update rate of the target network\n",
    "TAU = 0.005\n",
    "\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "LR = 1e-4\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "# Get num of actions from gym action space\n",
    "n_actions = env.action_space.n # left or right\n",
    "# Get the number of state observations\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# A nn which takes an observation of env as input and outputs a probability over possible actions\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "# the target_net is a copy of the policy net which stabilizes the learning process to ensure that no destructive changes are made to the model\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with pt.no_grad():\n",
    "            # basically return the largest column in each row.\n",
    "            # Second column on max result is index of where max element was found\n",
    "            # So we pick action with the largest expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1,1)\n",
    "    else:\n",
    "        # returns a random sample\n",
    "        return pt.tensor([[env.action_space.sample()]], device=device, dtype=pt.long)\n",
    "\n",
    "# an episode is basically an entire step through the learning process\n",
    "episode_durations = []\n",
    "\n",
    "# continue with plot_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
